(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{293:function(t,s,a){"use strict";a.r(s);var n=a(14),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"开始使用-canary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#开始使用-canary"}},[t._v("#")]),t._v(" 开始使用 Canary")]),t._v(" "),s("p",[t._v("欢迎来到Canary模型对抗鲁棒性评估框架学习教程！")]),t._v(" "),s("p",[t._v("在本章节中，我们将使用 "),s("code",[t._v("Canary")]),t._v(" 和 "),s("code",[t._v("PyTorch")]),t._v(" 构建一个简单的模型鲁棒性测试任务。值得注意的是，我们在"),s("code",[t._v("Canary Library")]),t._v("提供了大量攻击方法和预训练模型，使用"),s("code",[t._v("Canary Library")]),t._v("可以避免重复造轮子，并极大的减少我们的工作量。"),s("strong",[t._v("但我们希望在本章节进行一个相对完整的演示，以完整展示Canary框架的基本功能与运行逻辑，因此我们将不使用任何由 "),s("code",[t._v("Canary Library")]),t._v(" 提供的攻击方法或模型。")])]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("我们的小建议")]),t._v(" "),s("p",[t._v("但我们仍然怀疑这篇详细的教程可能对我们的新朋友们并不友好，大家或许更倾向于尝试运行代码而不是面对冗杂的模型与攻击方法准备、集成和调试，"),s("strong",[t._v("因为这可能使您的头发数量-10086")]),t._v("。所以如果您确实是这样想的，那么请不要继续阅读本教程的其他内容，让我们转到 "),s("RouterLink",{attrs:{to:"/start/get-started-with-canary-sefi-use-library.html"}},[t._v("开始使用 Canary Library")]),t._v(" 继续阅读。")],1)]),t._v(" "),s("p",[t._v("如果您拒绝我们的小建议，那么，"),s("strong",[t._v("勇者")]),t._v("！请让我们 "),s("s",[t._v("原神启动")]),t._v(" 吧！")]),t._v(" "),s("h2",{attrs:{id:"第0步-准备"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#第0步-准备"}},[t._v("#")]),t._v(" 第0步：准备")]),t._v(" "),s("p",[t._v("在开始编写任何实际代码之前，让我们确保我们已经做好了一切必要的准备。")]),t._v(" "),s("h3",{attrs:{id:"安装依赖项"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#安装依赖项"}},[t._v("#")]),t._v(" 安装依赖项")]),t._v(" "),s("p",[t._v("我们安装 "),s("code",[t._v("PyTorch")]),t._v(" (和"),s("code",[t._v("Torchvision")]),t._v(") 和 "),s("code",[t._v("Canary")]),t._v(" 所需的软件包：")]),t._v(" "),s("div",{staticClass:"language-sh extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sh"}},[s("code",[t._v("pip "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" torch torchvision torchaudio\npip "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-r")]),t._v(" requirement.txt\n")])])]),s("p",[t._v("* 为确保"),s("code",[t._v("Canary Library")]),t._v("项目可用，我们推荐"),s("code",[t._v("PyTorch")]),t._v("的版本应至少 ≥ 2.0.0")]),t._v(" "),s("h3",{attrs:{id:"准备数据集与模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#准备数据集与模型"}},[t._v("#")]),t._v(" 准备数据集与模型")]),t._v(" "),s("p",[t._v("在本教程中，我们通过在流行的"),s("code",[t._v("CIFAR-10")]),t._v("数据集上训练的简单卷积神经网络"),s("code",[t._v("CNN")]),t._v("来介绍对抗鲁棒性评估。")]),t._v(" "),s("p",[t._v("我们假设您已经足够熟练的使用"),s("code",[t._v("PyTorch")]),t._v("，因此不会详细介绍与"),s("code",[t._v("PyTorch")]),t._v("相关的方面。如果您想更深入地了解"),s("code",[t._v("PyTorch")]),t._v("，我们建议您参考 "),s("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("使用 PYTORCH 进行深度学习：60 分钟闪电战"),s("OutboundLink")],1),t._v("。")]),t._v(" "),s("p",[t._v("我们可以使用"),s("code",[t._v("Torchvision")]),t._v("中自带的数据集"),s("code",[t._v("CIFAR-10")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainset "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CIFAR10"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"workspace/dataset/CIFAR10"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" train"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" download"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" transform"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("transform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("我们使用这一数据集训练"),s("code",[t._v("PyTorch")]),t._v("教程中描述的简单"),s("code",[t._v("CNN")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Net")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pool "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MaxPool2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("120")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("120")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("84")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc3 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("84")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("view"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc3"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x\n")])])]),s("p",[t._v("我们假设您已经完成了训练工作，此时我们保存模型权重，即得到了一个模型的预训练权重文件"),s("code",[t._v("net.pth")]),t._v("。您应当确保以下过程是可以正确执行的：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 预处理图片")]),t._v("\nimg "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255.0")]),t._v("\nimg "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nimg "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nimg "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载模型")]),t._v("\nnet "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nnet"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_state_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workspace/model/net.pth'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 推理")]),t._v("\noutputs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 结果处理")]),t._v("\nresults "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("outputs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("detach"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cpu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresults "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 打印结果")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"准备对抗攻击-adversarial-attack-方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#准备对抗攻击-adversarial-attack-方法"}},[t._v("#")]),t._v(" 准备对抗攻击(Adversarial Attack)方法")]),t._v(" "),s("p",[t._v("在本教程中，我们通过复现"),s("code",[t._v("Goodfellow")]),t._v("等人发表在"),s("code",[t._v("ICLR2015")]),t._v("会议上的"),s("code",[t._v("Fast Gradient Sign Method/FGSM")]),t._v("算法来介绍对抗鲁棒性评估。")]),t._v(" "),s("p",[t._v("在白盒环境下，"),s("code",[t._v("FGSM")]),t._v("通过求出模型对输入的导数，然后用符号函数得到其具体的梯度方向，沿着梯度方向行进一个步长，即可得到“对抗扰动”，将其叠加在原输入上即得到了对抗样本。")]),t._v(" "),s("p",[t._v("我们复现"),s("code",[t._v("FGSM")]),t._v("的一个迭代版本（"),s("code",[t._v("I-FGSM")]),t._v("）如下：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("I_FGSM")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clip_min"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clip_max"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epsilon"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 待攻击的白盒模型")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" T  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 迭代攻击轮数")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epsilon "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" epsilon  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以无穷范数作为约束，设置最大值")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_min "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clip_min  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 像素值的下限")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_max "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clip_max  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 像素值的上限")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("attack")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 损失函数")]),t._v("\n        loss_ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CrossEntropyLoss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 克隆原始数据")]),t._v("\n        ori_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clone"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 定义图片可获取梯度")]),t._v("\n        img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("requires_grad "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 迭代攻击")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("iter")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型预测")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            output "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 计算loss，非靶向攻击")]),t._v("\n            loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ori_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 反向传播")]),t._v("\n            loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            grad "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data\n            img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("grad "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 更新图像像素")]),t._v("\n            img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epsilon "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sign"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("grad"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" img\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将图片进行clip")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("clip_value")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ori_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clamp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" ori_x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epsilon"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epsilon"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" ori_x\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clamp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_min"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_max"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data\n")])])]),s("h2",{attrs:{id:"第1步-委托模型、攻击方法至canary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#第1步-委托模型、攻击方法至canary"}},[t._v("#")]),t._v(" 第1步：委托模型、攻击方法至Canary")]),t._v(" "),s("p",[t._v("接下来，我们将已准备完成的模型和攻击方法集成至"),s("code",[t._v("Canary")]),t._v("。"),s("code",[t._v("Canary")]),t._v("使用一组装饰器，以收集各个组件（如模型、攻击防御算法和数据集加载器），其中，与模型有关的装饰器如下：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("model")]),t._v(" - 装饰一个模型生成函数\n"),s("ul",[s("li",[s("strong",[t._v("name")]),t._v(" - 模型名称。")])])]),t._v(" "),s("li",[s("strong",[t._v("util")]),t._v(" - 装饰一个工具组件函数，其中"),s("code",[t._v("util")]),t._v("装饰器接收以下参数以标记函数的具体作用：\n"),s("ul",[s("li",[s("strong",[t._v("util_type")]),t._v(" - 工具类型：一个"),s("code",[t._v("SubComponentType")]),t._v("枚举值。其中与模型相关的类型有：\n"),s("ul",[s("li",[t._v("图片预处理器 "),s("code",[t._v("IMG_PREPROCESSOR")]),t._v("、")]),t._v(" "),s("li",[t._v("图片逆处理器 "),s("code",[t._v("IMG_REVERSE_PROCESSOR")]),t._v("、")]),t._v(" "),s("li",[t._v("结果处理器 "),s("code",[t._v("RESULT_POSTPROCESSOR")]),t._v("、")]),t._v(" "),s("li",[t._v("模型推理器 "),s("code",[t._v("MODEL_INFERENCE_DETECTOR")]),t._v("；")])])]),t._v(" "),s("li",[s("strong",[t._v("util_target")]),t._v(" - 工具目标：一个"),s("code",[t._v("ComponentType")]),t._v("枚举值。此处我们将其设置为"),s("code",[t._v("MODEL")]),t._v("，意味着该工具组件函数是为模型服务的；")]),t._v(" "),s("li",[s("strong",[t._v("name")]),t._v(" - 该工具组件绑定的目标模型名称。")])])])]),t._v(" "),s("p",[t._v("与攻击方法有关的装饰器如下：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("attacker_class")]),t._v(" - 装饰一个攻击方法类\n"),s("ul",[s("li",[s("strong",[t._v("name")]),t._v(" - 攻击方法名称。")])])]),t._v(" "),s("li",[s("strong",[t._v("attack")]),t._v(" - 装饰一个攻击方法函数\n"),s("ul",[s("li",[s("strong",[t._v("name")]),t._v(" - 攻击方法名称；")]),t._v(" "),s("li",[s("strong",[t._v("is_inclass")]),t._v(" - 该攻击方法函数是否属于一个攻击方法类。如果装饰的函数在一个攻击方法类中，则该项必须为"),s("code",[t._v("True")]),t._v("，否则为"),s("code",[t._v("False")]),t._v("；")]),t._v(" "),s("li",[t._v("其他参数暂时不做额外介绍。")])])])]),t._v(" "),s("h3",{attrs:{id:"新建工程并构建目录"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#新建工程并构建目录"}},[t._v("#")]),t._v(" 新建工程并构建目录")]),t._v(" "),s("p",[t._v("首先，我们新建一个目录结构：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v(".\n├── model.py\n├── attack.py\n├── run.py\n├── config.json\n└── Canary_SEFI\n")])])]),s("p",[t._v("我们在"),s("code",[t._v("model.py")]),t._v("和"),s("code",[t._v("attack.py")]),t._v("中都初始化一个"),s("code",[t._v("SEFIComponent()")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("component_decorator "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SEFIComponent\nsefi_component "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SEFIComponent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"集成模型生成函数至canary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#集成模型生成函数至canary"}},[t._v("#")]),t._v(" 集成模型生成函数至Canary")]),t._v(" "),s("p",[s("strong",[t._v("👇请将该部分存放在"),s("code",[t._v("model.py")]),t._v("中👇")])]),t._v(" "),s("p",[t._v("我们需要构建一个模型生成函数，以正确加载模型；然后我们使用"),s("code",[t._v("@sefi_component.model")]),t._v("进行装饰：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("create_model")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("run_device"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型运行位置")]),t._v("\n    run_device "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" run_device "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" run_device "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cuda'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cuda"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_available"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cpu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载模型")]),t._v("\n    net "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_state_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'workspace/model/net.pth'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("run_device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" net\n")])])]),s("h3",{attrs:{id:"集成模型图片处理方法函数至canary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#集成模型图片处理方法函数至canary"}},[t._v("#")]),t._v(" 集成模型图片处理方法函数至Canary")]),t._v(" "),s("p",[s("strong",[t._v("👇请将该部分存放在"),s("code",[t._v("model.py")]),t._v("中👇")])]),t._v(" "),s("p",[t._v("从第0步我们准备的模型测试代码中可以看出，一张图片若想被模型正确处理，需要进行"),s("code",[t._v("图片预处理")]),t._v("👉"),s("code",[t._v("加载模型")]),t._v("👉"),s("code",[t._v("推理")]),t._v("👉"),s("code",[t._v("结果处理")]),t._v("四个阶段，尽管有些阶段是可选的。因此，我们需要构建以下函数，并使用@sefi_component.util进行装饰：")]),t._v(" "),s("p",[t._v("图片预处理函数：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("util")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("util_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("SubComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMG_PREPROCESSOR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" util_target"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("img_pre_handler")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ori_imgs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    run_device "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"run_device"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cuda'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cuda"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("is_available"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cpu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" ori_img "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" ori_imgs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        ori_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("copy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 预处理代码")]),t._v("\n        ori_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255.0")]),t._v("\n        ori_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        ori_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Variable"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("run_device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        ori_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ori_img "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" result "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ori_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n")])])]),s("p",[t._v("该函数接收两个参数"),s("code",[t._v("ori_imgs")]),t._v("和"),s("code",[t._v("args")]),t._v("：")]),t._v(" "),s("ul",[s("li",[t._v("其中"),s("code",[t._v("ori_imgs")]),t._v("是一个由数据集中读取的"),s("code",[t._v("numpy.ndarray")]),t._v("类型的图片，其形状为彩色图片的"),s("code",[t._v("[W×H×3]")]),t._v("或灰度图片的"),s("code",[t._v("[W×H×1]")]),t._v("；")]),t._v(" "),s("li",[s("code",[t._v("args")]),t._v("是图片与结果处理器的共用配置参数，由用户自行传入，在本例中为空。")])]),t._v(" "),s("p",[t._v("推理函数：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("util")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("util_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("SubComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MODEL_INFERENCE_DETECTOR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" util_target"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("inference_detector")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("该函数接收两个参数"),s("code",[t._v("model")]),t._v("和"),s("code",[t._v("img")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("model")]),t._v("是模型生成函数"),s("code",[t._v("create_model()")]),t._v("函数的输出结果；")]),t._v(" "),s("li",[s("code",[t._v("img")]),t._v("是模型预处理函数"),s("code",[t._v("img_pre_handler()")]),t._v("函数的输出结果。")])]),t._v(" "),s("p",[t._v("模型结果处理函数：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("util")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("util_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("SubComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("RESULT_POSTPROCESSOR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" util_target"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("result_post_handler")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    results "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functional"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("logits"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("detach"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cpu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    predicts "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" result "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" results"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        predicts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("argmax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" predicts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" results\n")])])]),s("p",[t._v("该函数接收两个参数"),s("code",[t._v("logits")]),t._v("和"),s("code",[t._v("args")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("logits")]),t._v("是推理函数"),s("code",[t._v("inference_detector()")]),t._v("函数的输出结果；")]),t._v(" "),s("li",[s("code",[t._v("args")]),t._v("是图片与结果处理器的共用配置参数，由用户自行传入，在本例中为空。")])]),t._v(" "),s("p",[t._v("在产生对抗样本后，我们需要将对抗样本保存为图片。由于生成对抗样本时的图片已经进行了预处理，因此我们需要用户定义一个逆转预处理的过程，以还原为原始图像。需要构建以下图片逆处理函数：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("util")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("util_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("SubComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("IMG_REVERSE_PROCESSOR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" util_target"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("ComponentType"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MODEL"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("img_post_handler")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adv_imgs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adv_imgs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        adv_imgs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adv_imgs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cpu"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" adv_img "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" adv_imgs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 逆处理代码")]),t._v("\n        adv_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adv_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        adv_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" adv_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255.0")]),t._v("\n        adv_img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adv_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("float32"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        result"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("adv_img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n")])])]),s("p",[t._v("该函数接收两个参数"),s("code",[t._v("adv_imgs")]),t._v("和"),s("code",[t._v("args")]),t._v("：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("adv_imgs")]),t._v("是攻击方法函数"),s("code",[t._v("attack()")]),t._v("函数（见下）的输出结果；")]),t._v(" "),s("li",[s("code",[t._v("args")]),t._v("是图片与结果处理器的共用配置参数，由用户自行传入，在本例中为空。")])]),t._v(" "),s("h3",{attrs:{id:"集成攻击方法函数至canary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#集成攻击方法函数至canary"}},[t._v("#")]),t._v(" 集成攻击方法函数至Canary")]),t._v(" "),s("p",[s("strong",[t._v("👇请将该部分存放在"),s("code",[t._v("attack.py")]),t._v("中👇")])]),t._v(" "),s("p",[t._v("我们需要将攻击方法委托至模型。我们使用"),s("code",[t._v("@sefi_component.attacker_class")]),t._v("装饰这个方法类，并使用"),s("code",[t._v("@sefi_component.attack")]),t._v("装饰这个攻击方法函数：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attacker_class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("attack_name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"MI_FGSM"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("I_FGSM")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" run_device"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" attack_type"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'UNTARGETED'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clip_min"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clip_max"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" T"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epsilon"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 待攻击的白盒模型")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" run_device\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" T  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 迭代攻击轮数")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("epsilon "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" epsilon  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以无穷范数作为约束，设置最大值")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_min "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clip_min  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 像素值的下限")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clip_max "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" clip_max  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 像素值的上限")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@sefi_component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attack")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I_FGSM"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" is_inclass"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("attack")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ori_labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tlabels"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" img\n")])])]),s("p",[s("strong",[t._v("请注意，与第0步中所示的攻击方法类"),s("code",[t._v("I_FGSM")]),t._v("略有区别")]),t._v("，本例中我们"),s("strong",[t._v("在攻击方法类的"),s("code",[t._v("__init__")]),t._v("函数中增加了攻击方法类型"),s("code",[t._v("attack_type")]),t._v("参数")]),t._v("，对于攻击方法类来说，这是必须接收的两个参数，如果该攻击方法不支持目标攻击，则可不使用以上参数。")]),t._v(" "),s("p",[t._v("同样的，我们"),s("strong",[t._v("在攻击方法函数"),s("code",[t._v("attack()")]),t._v("函数中增加了原始标签"),s("code",[t._v("ori_labels")]),t._v("和目标标签"),s("code",[t._v("tlabel")]),t._v("两个参数")]),t._v("，对于攻击方法函数来说，这是必须接收的两个参数，可不使用以上参数。")]),t._v(" "),s("p",[t._v("攻击方法类"),s("code",[t._v("I_FGSM")]),t._v("的"),s("code",[t._v("__init__")]),t._v("函数接收一组参数，其中"),s("code",[t._v("model")]),t._v("、"),s("code",[t._v("run_device")]),t._v("和"),s("code",[t._v("attack_type")]),t._v("参数是必选参数，其余参数由用户任意指定，并在后续配置中配置即可：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("model")]),t._v("是模型生成函数"),s("code",[t._v("create_model()")]),t._v("函数的输出结果；")]),t._v(" "),s("li",[s("code",[t._v("run_device")]),t._v("是运行设备，一般为"),s("code",[t._v("cpu")]),t._v("或"),s("code",[t._v("cuda")]),t._v("；")]),t._v(" "),s("li",[s("code",[t._v("attack_type")]),t._v("是攻击方法类型，仅有"),s("code",[t._v("TARGETED")]),t._v("与"),s("code",[t._v("UNTARGETED")]),t._v("两种取值。")])]),t._v(" "),s("p",[s("code",[t._v("attack()")]),t._v("函数接收三个参数"),s("code",[t._v("img")]),t._v("、"),s("code",[t._v("ori_labels")]),t._v("和"),s("code",[t._v("tlabel")]),t._v("参数：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("img")]),t._v("是模型预处理函数"),s("code",[t._v("img_pre_handler()")]),t._v("函数的输出结果；")]),t._v(" "),s("li",[s("code",[t._v("ori_labels")]),t._v("是数据集标注的图片标签（"),s("code",[t._v("Array")]),t._v("数组）；")]),t._v(" "),s("li",[s("code",[t._v("tlabel")]),t._v("是目标攻击标签（"),s("code",[t._v("Array")]),t._v("数组），该数组仅当随机目标选型选用，且类初始化时"),s("code",[t._v("attack_type")]),t._v("被设为"),s("code",[t._v("TARGETED")]),t._v("时才会传入。")])]),t._v(" "),s("p",[s("code",[t._v("attack()")]),t._v("函数产生对抗样本图片，该图片将交由图片逆处理函数"),s("code",[t._v("img_post_handler()")]),t._v("处理。")]),t._v(" "),s("h2",{attrs:{id:"第2步-配置canary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#第2步-配置canary"}},[t._v("#")]),t._v(" 第2步：配置Canary")]),t._v(" "),s("p",[t._v("现在，您已经将由您自行提供的模型、攻击方法都集成至"),s("code",[t._v("Canary")]),t._v("了，接下来我们将开始构建一个测试任务。")]),t._v(" "),s("p",[s("strong",[t._v("👇请将该部分存放在"),s("code",[t._v("run.py")]),t._v("中👇")])]),t._v(" "),s("p",[t._v("我们首先引入必要依赖，并加载模型和攻击方法至"),s("code",[t._v("Canary")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" random\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("function"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("enum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multi_db_mode_enum "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MultiDatabaseMode\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("function"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("helper"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multi_db "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" use_multi_database\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("function"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("helper"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("recovery "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" global_recovery\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("service"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("security_evaluation "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SecurityEvaluation\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("task_manager "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" task_manager\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" canary_sefi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("component"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("component_manager "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SEFI_component_manager\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载攻击方法")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attack "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sefi_component "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" ifgsm_attacker\nSEFI_component_manager"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ifgsm_attacker"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 加载模型")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sefi_component "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" net\nSEFI_component_manager"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("net"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("接下来，我们构建配置：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("example_config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集配置")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataset_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 用于测试的图片数量")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataset"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataset_name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CIFAR10"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集名称，此处如果是Torchvision定义的数据集会自动加载")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataset_path"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"workspace/dataset/CIFAR10"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集路径")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataset_type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TEST"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集类型")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"n_classes"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集类数量")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"is_gray"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集是否是灰度图")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 数据集随机选取图片的种子")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dataset_seed"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randint"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型配置")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_list"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型名，本例中模型名是Net")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"inference_batch_config"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型预测的 Batch 数")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ResNet(CIFAR-10)"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 攻击方法配置")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"attacker_list"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I_FGSM"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 攻击方法名，本例中攻击方法名是I_FGSM")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Net"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 攻击方法攻击的目标模型")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"attacker_config"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 攻击配置参数")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I_FGSM"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这是I_FGSM推荐的攻击参数")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"clip_min"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"clip_max"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"T"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"attack_type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"UNTARGETED"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"epsilon"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adv_example_generate_batch_config"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模型生成对抗样本的 Batch 数")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"I_FGSM"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ResNet(CIFAR-10)"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 转移测试模式：本例中我们只选择了一个模型，不存在转移测试，因此为NOT")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transfer_attack_test_mode"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"NOT"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h2",{attrs:{id:"第3步-canary启动"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#第3步-canary启动"}},[t._v("#")]),t._v(" 第3步：Canary启动")]),t._v(" "),s("p",[t._v("我们需要更改一下"),s("code",[t._v("Canary")]),t._v("的系统配置，并将以下内容存入 "),s("code",[t._v("config.json")]),t._v("（如果没有）。在本例中，我们只需要关注"),s("code",[t._v("datasetPath")]),t._v("和"),s("code",[t._v("baseTempPath")]),t._v("，它们分别是数据集路径和临时文件路径。")]),t._v(" "),s("p",[t._v("如果您不打算使用"),s("code",[t._v("Canary WebView")]),t._v("，"),s("code",[t._v("appName")]),t._v("、"),s("code",[t._v("appDesc")]),t._v("对您毫无意义，完全可以不必填写。")]),t._v(" "),s("div",{staticClass:"language-json extra-class"},[s("pre",{pre:!0,attrs:{class:"language-json"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"appName"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CANARY Test"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"appDesc"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is an example program to start test using Canary SEFI"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"datasetPath"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/workplace/dataset/"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"baseTempPath"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/workplace/temp/"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"centerDatabasePath"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/workplace/temp/"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"system"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"limited_read_img_size"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("900")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"use_file_memory_cache"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"save_fig_model"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"save_img_file"')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("最后，我们使用该配置启动 "),s("s",[t._v("原神")]),t._v(" "),s("code",[t._v("Canary")]),t._v("运行评估测试：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 初始化任务，使用显卡CUDA设备运行任务")]),t._v("\n    task_manager"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("init_task"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("show_logo"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" run_device"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cuda"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设置当前模式为简单数据库模式（非高级用户请勿修改此设置）")]),t._v("\n    use_multi_database"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mode"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("MultiDatabaseMode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SIMPLE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用配置构建评估任务并启动")]),t._v("\n    security_evaluation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SecurityEvaluation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("example_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    security_evaluation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("attack_full_test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"最后提示"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#最后提示"}},[t._v("#")]),t._v(" 最后提示")]),t._v(" "),s("p",[t._v("恭喜，您刚刚使用"),s("code",[t._v("I-FGSM")]),t._v("对"),s("code",[t._v("CNN")]),t._v("模型进行了一次对抗攻击，并评估了该模型的鲁棒性与攻击方法的有效性。您所看到的相同方法可以用于其他深度学习模型（不仅仅是基于"),s("code",[t._v("CIFAR-10")]),t._v("训练的简单"),s("code",[t._v("CNN")]),t._v("）和攻击方法（不仅仅是"),s("code",[t._v("I-FGSM")]),t._v("）")]),t._v(" "),s("p",[t._v("在下一章中，我们将介绍一些更易于使用的方法。不想自己实现主流的攻击和防御方法？不想自己训练用于测试的标准模型？我们将在下一个教程中介绍所有这些以及更多内容。")])])}),[],!1,null,null,null);s.default=e.exports}}]);